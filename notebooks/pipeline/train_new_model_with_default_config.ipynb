{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/papermill/iorw.py:50: FutureWarning: pyarrow.HadoopFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n",
      "  from pyarrow import HadoopFileSystem\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import scrapbook as sb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "src_dir = \"../../src\"  # Ignore this!\n",
    "\n",
    "# Pipeline settings\n",
    "task = \"sequence_classification\"  # chinese_word_segmentation OR target_classification, OR sequence_classification\n",
    "device = 0\n",
    "model_params = {'num_train_epochs': 5}\n",
    "model_dir = f\"../output/test_pipeline_{task}_tmp\"  # output dir for new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(src_dir)\n",
    "from pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '<p> zaia係咩, 我都想睇水舞間, 朋友睇完都話好睇!', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_raw_data = json.load(open(f\"../data/datasets/sample/{task}/train_sample.json\", 'r'))\n",
    "dev_raw_data = json.load(open(f\"../data/datasets/sample/{task}/train_sample.json\", 'r'))\n",
    "test_raw_data = json.load(open(f\"../data/datasets/sample/{task}/train_sample.json\", 'r'))\n",
    "print(train_raw_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-26 01:29:44 ***** Model class is not specified for sequence_classification. *****\n",
      "2021-11-26 01:29:44   Default model = BERT_CLS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../config/examples/sequence_classification/BERT_CLS\n",
      "['.ipynb_checkpoints', 'run.yaml', 'model', 'result', 'logs']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline = Pipeline(\n",
    "    task=task, \n",
    "    device=device, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-26 01:29:45 ***** Initializing pipeline *****\n",
      "2021-11-26 01:29:45 ***** Loading tokenizer *****\n",
      "2021-11-26 01:29:45   Tokenizer source = 'transformers'\n",
      "2021-11-26 01:29:48 ***** Initializing model *****\n",
      "2021-11-26 01:29:48   Task = sequence_classification\n",
      "2021-11-26 01:29:48   Model class = BERT_CLS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenizer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-26 01:29:49 ***** Loading pretrained language model *****\n",
      "2021-11-26 01:29:49   Pretrained BERT = 'bert-base-chinese'\n",
      "2021-11-26 01:29:59 ***** Loading data *****\n",
      "2021-11-26 01:29:59   Raw data is provided.\n",
      "3it [00:00, 105.36it/s]\n",
      "2021-11-26 01:29:59   Loaded samples = 3\n",
      "2021-11-26 01:29:59 ***** Loading data *****\n",
      "2021-11-26 01:29:59   Raw data is provided.\n",
      "3it [00:00, 134.71it/s]\n",
      "2021-11-26 01:29:59   Loaded samples = 3\n",
      "2021-11-26 01:29:59 ***** Running training *****\n",
      "2021-11-26 01:29:59   Num examples = 3\n",
      "2021-11-26 01:29:59   Num Epochs = 5\n",
      "2021-11-26 01:29:59   Sampler = \n",
      "2021-11-26 01:29:59   Batch size = 32\n",
      "2021-11-26 01:29:59   Gradient Accumulation steps = 1\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s, tr_loss=1.17]\u001b[A\n",
      "Iteration: 100%|██████████| 1/1 [00:00<00:00,  7.87it/s, tr_loss=1.17]\u001b[A\n",
      "2021-11-26 01:29:59 ***** Epoch end: 0 *****\n",
      "2021-11-26 01:29:59 ***** Running evaluation *****\n",
      "2021-11-26 01:29:59   Num examples = 3\n",
      "2021-11-26 01:29:59   Batch size = 64\n",
      "\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 47.34it/s]\n",
      "2021-11-26 01:29:59   accuracy = 1.0\n",
      "2021-11-26 01:29:59   macro_f1 = 1.0\n",
      "2021-11-26 01:29:59   micro_f1 = 1.0\n",
      "2021-11-26 01:29:59   support = 3\n",
      "2021-11-26 01:29:59   -1-precision = 1.0\n",
      "2021-11-26 01:29:59   -1-recall = 1.0\n",
      "2021-11-26 01:29:59   -1-f1-score = 1.0\n",
      "2021-11-26 01:29:59   -1-support = 1\n",
      "2021-11-26 01:29:59   0-precision = 1.0\n",
      "2021-11-26 01:29:59   0-recall = 1.0\n",
      "2021-11-26 01:29:59   0-f1-score = 1.0\n",
      "2021-11-26 01:29:59   0-support = 1\n",
      "2021-11-26 01:29:59   1-precision = 1.0\n",
      "2021-11-26 01:29:59   1-recall = 1.0\n",
      "2021-11-26 01:29:59   1-f1-score = 1.0\n",
      "2021-11-26 01:29:59   1-support = 1\n",
      "2021-11-26 01:29:59   loss = 0.8576257824897766\n",
      "2021-11-26 01:29:59   dataset = dev\n",
      "Epoch:  20%|██        | 1/5 [00:00<00:00,  5.51it/s]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s, tr_loss=0.984]\u001b[A\n",
      "Iteration: 100%|██████████| 1/1 [00:00<00:00,  8.90it/s, tr_loss=0.984]\u001b[A\n",
      "2021-11-26 01:29:59 ***** Epoch end: 1 *****\n",
      "2021-11-26 01:29:59 ***** Running evaluation *****\n",
      "2021-11-26 01:29:59   Num examples = 3\n",
      "2021-11-26 01:29:59   Batch size = 64\n",
      "\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 47.59it/s]\n",
      "2021-11-26 01:29:59   accuracy = 1.0\n",
      "2021-11-26 01:29:59   macro_f1 = 1.0\n",
      "2021-11-26 01:29:59   micro_f1 = 1.0\n",
      "2021-11-26 01:29:59   support = 3\n",
      "2021-11-26 01:29:59   -1-precision = 1.0\n",
      "2021-11-26 01:29:59   -1-recall = 1.0\n",
      "2021-11-26 01:29:59   -1-f1-score = 1.0\n",
      "2021-11-26 01:29:59   -1-support = 1\n",
      "2021-11-26 01:29:59   0-precision = 1.0\n",
      "2021-11-26 01:29:59   0-recall = 1.0\n",
      "2021-11-26 01:29:59   0-f1-score = 1.0\n",
      "2021-11-26 01:29:59   0-support = 1\n",
      "2021-11-26 01:29:59   1-precision = 1.0\n",
      "2021-11-26 01:29:59   1-recall = 1.0\n",
      "2021-11-26 01:29:59   1-f1-score = 1.0\n",
      "2021-11-26 01:29:59   1-support = 1\n",
      "2021-11-26 01:29:59   loss = 0.7209531664848328\n",
      "2021-11-26 01:29:59   dataset = dev\n",
      "Epoch:  40%|████      | 2/5 [00:00<00:00,  5.74it/s]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 1/1 [00:00<00:00, 10.19it/s, tr_loss=1.12]\n",
      "2021-11-26 01:29:59 ***** Epoch end: 2 *****\n",
      "2021-11-26 01:29:59 ***** Running evaluation *****\n",
      "2021-11-26 01:29:59   Num examples = 3\n",
      "2021-11-26 01:29:59   Batch size = 64\n",
      "\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 47.16it/s]\n",
      "2021-11-26 01:29:59   accuracy = 1.0\n",
      "2021-11-26 01:29:59   macro_f1 = 1.0\n",
      "2021-11-26 01:29:59   micro_f1 = 1.0\n",
      "2021-11-26 01:29:59   support = 3\n",
      "2021-11-26 01:29:59   -1-precision = 1.0\n",
      "2021-11-26 01:29:59   -1-recall = 1.0\n",
      "2021-11-26 01:29:59   -1-f1-score = 1.0\n",
      "2021-11-26 01:29:59   -1-support = 1\n",
      "2021-11-26 01:29:59   0-precision = 1.0\n",
      "2021-11-26 01:29:59   0-recall = 1.0\n",
      "2021-11-26 01:29:59   0-f1-score = 1.0\n",
      "2021-11-26 01:29:59   0-support = 1\n",
      "2021-11-26 01:29:59   1-precision = 1.0\n",
      "2021-11-26 01:29:59   1-recall = 1.0\n",
      "2021-11-26 01:29:59   1-f1-score = 1.0\n",
      "2021-11-26 01:29:59   1-support = 1\n",
      "2021-11-26 01:29:59   loss = 0.62933748960495\n",
      "2021-11-26 01:29:59   dataset = dev\n",
      "Epoch:  60%|██████    | 3/5 [00:00<00:00,  6.07it/s]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 1/1 [00:00<00:00, 10.12it/s, tr_loss=0.863]\n",
      "2021-11-26 01:30:00 ***** Epoch end: 3 *****\n",
      "2021-11-26 01:30:00 ***** Running evaluation *****\n",
      "2021-11-26 01:30:00   Num examples = 3\n",
      "2021-11-26 01:30:00   Batch size = 64\n",
      "\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 45.41it/s]\n",
      "2021-11-26 01:30:00   accuracy = 1.0\n",
      "2021-11-26 01:30:00   macro_f1 = 1.0\n",
      "2021-11-26 01:30:00   micro_f1 = 1.0\n",
      "2021-11-26 01:30:00   support = 3\n",
      "2021-11-26 01:30:00   -1-precision = 1.0\n",
      "2021-11-26 01:30:00   -1-recall = 1.0\n",
      "2021-11-26 01:30:00   -1-f1-score = 1.0\n",
      "2021-11-26 01:30:00   -1-support = 1\n",
      "2021-11-26 01:30:00   0-precision = 1.0\n",
      "2021-11-26 01:30:00   0-recall = 1.0\n",
      "2021-11-26 01:30:00   0-f1-score = 1.0\n",
      "2021-11-26 01:30:00   0-support = 1\n",
      "2021-11-26 01:30:00   1-precision = 1.0\n",
      "2021-11-26 01:30:00   1-recall = 1.0\n",
      "2021-11-26 01:30:00   1-f1-score = 1.0\n",
      "2021-11-26 01:30:00   1-support = 1\n",
      "2021-11-26 01:30:00   loss = 0.5771722793579102\n",
      "2021-11-26 01:30:00   dataset = dev\n",
      "Epoch:  80%|████████  | 4/5 [00:00<00:00,  6.30it/s]\n",
      "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 1/1 [00:00<00:00, 10.49it/s, tr_loss=0.673]\n",
      "2021-11-26 01:30:00 ***** Epoch end: 4 *****\n",
      "2021-11-26 01:30:00 ***** Running evaluation *****\n",
      "2021-11-26 01:30:00   Num examples = 3\n",
      "2021-11-26 01:30:00   Batch size = 64\n",
      "\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 47.27it/s]\n",
      "2021-11-26 01:30:00   accuracy = 1.0\n",
      "2021-11-26 01:30:00   macro_f1 = 1.0\n",
      "2021-11-26 01:30:00   micro_f1 = 1.0\n",
      "2021-11-26 01:30:00   support = 3\n",
      "2021-11-26 01:30:00   -1-precision = 1.0\n",
      "2021-11-26 01:30:00   -1-recall = 1.0\n",
      "2021-11-26 01:30:00   -1-f1-score = 1.0\n",
      "2021-11-26 01:30:00   -1-support = 1\n",
      "2021-11-26 01:30:00   0-precision = 1.0\n",
      "2021-11-26 01:30:00   0-recall = 1.0\n",
      "2021-11-26 01:30:00   0-f1-score = 1.0\n",
      "2021-11-26 01:30:00   0-support = 1\n",
      "2021-11-26 01:30:00   1-precision = 1.0\n",
      "2021-11-26 01:30:00   1-recall = 1.0\n",
      "2021-11-26 01:30:00   1-f1-score = 1.0\n",
      "2021-11-26 01:30:00   1-support = 1\n",
      "2021-11-26 01:30:00   loss = 0.5523586273193359\n",
      "2021-11-26 01:30:00   dataset = dev\n",
      "Epoch: 100%|██████████| 5/5 [00:00<00:00,  6.52it/s]\n",
      "2021-11-26 01:30:00 ***** Training end *****\n",
      "2021-11-26 01:30:00   Model path = ../output/test_pipeline_sequence_classification_tmp2/model/model.pt\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BERT_CLS. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfOutput. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertIntermediate. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertOutput. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertPooler. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Tanh. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CrossEntropyLoss. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline.train(\n",
    "    model_dir, \n",
    "    train_raw_data=train_raw_data, \n",
    "    dev_raw_data=dev_raw_data, \n",
    "    model_params=model_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 02:14:51 ***** Loading data *****\n",
      "2021-11-25 02:14:51   Raw data is provided.\n",
      "3it [00:00, 114.58it/s]\n",
      "2021-11-25 02:14:51   Loaded samples = 3\n",
      "2021-11-25 02:14:51 ***** Running evaluation *****\n",
      "2021-11-25 02:14:51   Num examples = 3\n",
      "2021-11-25 02:14:51   Batch size = 64\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 45.95it/s]\n",
      "2021-11-25 02:14:51   accuracy = 1.0\n",
      "2021-11-25 02:14:51   macro_f1 = 1.0\n",
      "2021-11-25 02:14:51   micro_f1 = 1.0\n",
      "2021-11-25 02:14:51   support = 3\n",
      "2021-11-25 02:14:51   -1-precision = 1.0\n",
      "2021-11-25 02:14:51   -1-recall = 1.0\n",
      "2021-11-25 02:14:51   -1-f1-score = 1.0\n",
      "2021-11-25 02:14:51   -1-support = 1\n",
      "2021-11-25 02:14:51   0-precision = 1.0\n",
      "2021-11-25 02:14:51   0-recall = 1.0\n",
      "2021-11-25 02:14:51   0-f1-score = 1.0\n",
      "2021-11-25 02:14:51   0-support = 1\n",
      "2021-11-25 02:14:51   1-precision = 1.0\n",
      "2021-11-25 02:14:51   1-recall = 1.0\n",
      "2021-11-25 02:14:51   1-f1-score = 1.0\n",
      "2021-11-25 02:14:51   1-support = 1\n",
      "2021-11-25 02:14:51   loss = 0.7164847254753113\n",
      "2021-11-25 02:14:51   dataset = train\n",
      "2021-11-25 02:14:51 ***** Running evaluation *****\n",
      "2021-11-25 02:14:51   Num examples = 3\n",
      "2021-11-25 02:14:51   Batch size = 64\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 46.21it/s]\n",
      "2021-11-25 02:14:51   accuracy = 1.0\n",
      "2021-11-25 02:14:51   macro_f1 = 1.0\n",
      "2021-11-25 02:14:51   micro_f1 = 1.0\n",
      "2021-11-25 02:14:51   support = 3\n",
      "2021-11-25 02:14:51   -1-precision = 1.0\n",
      "2021-11-25 02:14:51   -1-recall = 1.0\n",
      "2021-11-25 02:14:51   -1-f1-score = 1.0\n",
      "2021-11-25 02:14:51   -1-support = 1\n",
      "2021-11-25 02:14:51   0-precision = 1.0\n",
      "2021-11-25 02:14:51   0-recall = 1.0\n",
      "2021-11-25 02:14:51   0-f1-score = 1.0\n",
      "2021-11-25 02:14:51   0-support = 1\n",
      "2021-11-25 02:14:51   1-precision = 1.0\n",
      "2021-11-25 02:14:51   1-recall = 1.0\n",
      "2021-11-25 02:14:51   1-f1-score = 1.0\n",
      "2021-11-25 02:14:51   1-support = 1\n",
      "2021-11-25 02:14:51   loss = 0.7164847254753113\n",
      "2021-11-25 02:14:51   dataset = dev\n",
      "2021-11-25 02:14:51 ***** Running evaluation *****\n",
      "2021-11-25 02:14:51   Num examples = 3\n",
      "2021-11-25 02:14:51   Batch size = 64\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 45.88it/s]\n",
      "2021-11-25 02:14:51   accuracy = 1.0\n",
      "2021-11-25 02:14:51   macro_f1 = 1.0\n",
      "2021-11-25 02:14:51   micro_f1 = 1.0\n",
      "2021-11-25 02:14:51   support = 3\n",
      "2021-11-25 02:14:51   -1-precision = 1.0\n",
      "2021-11-25 02:14:51   -1-recall = 1.0\n",
      "2021-11-25 02:14:51   -1-f1-score = 1.0\n",
      "2021-11-25 02:14:51   -1-support = 1\n",
      "2021-11-25 02:14:51   0-precision = 1.0\n",
      "2021-11-25 02:14:51   0-recall = 1.0\n",
      "2021-11-25 02:14:51   0-f1-score = 1.0\n",
      "2021-11-25 02:14:51   0-support = 1\n",
      "2021-11-25 02:14:51   1-precision = 1.0\n",
      "2021-11-25 02:14:51   1-recall = 1.0\n",
      "2021-11-25 02:14:51   1-f1-score = 1.0\n",
      "2021-11-25 02:14:51   1-support = 1\n",
      "2021-11-25 02:14:51   loss = 0.7164847254753113\n",
      "2021-11-25 02:14:51   dataset = test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics = pipeline.test(\n",
    "    test_raw_data=test_raw_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "{'content': '<p> zaia係咩, 我都想睇水舞間, 朋友睇完都話好睇!', 'label': 1}\n",
      "Output:\n",
      "{'prediction_id': 0, 'prediction': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\")\n",
    "print(test_raw_data[0])\n",
    "\n",
    "output = pipeline.predict(\n",
    "    data_dict=test_raw_data[0],\n",
    ")\n",
    "\n",
    "print(\"Output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": 0.9983041266252121,
       "encoder": "json",
       "name": "macro_f1",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "macro_f1"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": 0.9983041266252121,
       "encoder": "json",
       "name": "micro_f1",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "micro_f1"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.glue(\"macro_f1\", metrics['macro_f1'])\n",
    "sb.glue(\"micro_f1\", metrics['micro_f1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc-autonumbering": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
