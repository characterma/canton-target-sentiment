{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "From data augmentation experiment for Apple Care 2 dataset, highlighted factors can ensure the quality of unlabeled dataset which enhance the performance of model.\n",
    "\n",
    "Experiment details in [2022_01_13 biweekly discussion.pptx](https://jira.wisers.com:18090/download/attachments/82808396/2022_01_13%20biweekly%20discussion.pptx?version=1&modificationDate=1642063801000&api=v2)\n",
    "\n",
    "For code design please browse [Confluence Proposed Module](https://jira.wisers.com:18090/display/RES/Proposed+Module2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/Users/hinova/canton-target-sentiment/src'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../src/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Generation\n",
    "\n",
    "Model is required to predict labels for unlabel data in order to sample data. This step can be skipped if user already got trained model directory which exists:\n",
    "- model directory\n",
    "    - run.yaml\n",
    "    - model.yaml\n",
    "    - label_to_id.json\n",
    "    - model.pt\n",
    "\n",
    "After run below training process:\n",
    "- code 0: successful training\n",
    "- code 256: failed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f\"python run.py --config_dir='../config/examples/sequence_classification/BERT_AVG'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User can sample label ratio based on the ratio of train set, or request desired label ratio. It depends on existence of argument (either input this argument or not):\n",
    "\n",
    "- label_ratio (if required desired label ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user can sample label ratio based on the ratio of train set, or request desired label ratio\n",
    "\n",
    "# label_ratio = {'-1': 0.3, '0': 0.4, '1': 0.3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model directory, must include above files\n",
    "model_dir = '../config/examples/sequence_classification/BERT_AVG/model' # model inference\n",
    "\n",
    "# unlabel path (json file name included)\n",
    "unlabel_path = '../data/datasets/sample/sequence_classification/unlabeled_sample.json' \n",
    "\n",
    "# save directory\n",
    "save_dir = '../data/datasets/sample/sequence_classification'\n",
    "save_data_file = 'sampled_unlabel_data.json'\n",
    "save_logit_file = 'sampled_unlabel_logits.pkl'\n",
    "\n",
    "# sample size and certainty\n",
    "sample_size = 6 # required, integer and smaller than size of unlabeled data\n",
    "certainty = 0.36 # required, only select the data that max(p)>certainty, \n",
    "                # p is the predicted probabilities (0 - 1) over the label space\n",
    "                # default certainty is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from utils import load_yaml\n",
    "class arg():\n",
    "    def __init__(self, model_dir: str):\n",
    "        # run_yaml configuration\n",
    "        run_config = load_yaml(Path(model_dir) / \"run.yaml\")\n",
    "        self.task = run_config['task']\n",
    "        self.data_config = run_config['data']\n",
    "        self.data_dir = Path(self.data_config['data_dir'])\n",
    "        self.prepro_config = run_config['text_prepro']\n",
    "        self.eval_config = run_config['eval']\n",
    "        self.train_config = run_config['train']\n",
    "        self.device = run_config['device']\n",
    "        model_class = self.train_config['model_class']\n",
    "\n",
    "        # model_yaml configuration\n",
    "        self.model_config = load_yaml(Path(model_dir) / \"model.yaml\")[model_class]\n",
    "        self.model_config['pretrained_lm_from_prev'] = model_dir\n",
    "\n",
    "        # label_to_id mapping\n",
    "        with open(model_dir+'/label_to_id.json', 'rb') as outfile:\n",
    "            self.label_to_id = json.load(outfile)\n",
    "        self.label_to_id_inv = dict(zip(self.label_to_id.values(), self.label_to_id.keys()))\n",
    "\n",
    "        # model directory which model locates\n",
    "        self.model_dir = Path(model_dir)\n",
    "\n",
    "args = arg(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual arguments: \n",
      " {'task': 'sequence_classification', 'data_config': {'output_dir': '../config/examples/sequence_classification/BERT_AVG', 'data_dir': '../data/datasets/sample/sequence_classification', 'train': 'train_sample.json', 'dev': 'train_sample.json', 'test': 'train_sample.json'}, 'data_dir': PosixPath('../data/datasets/sample/sequence_classification'), 'prepro_config': {'steps': ['utf8_replace', 'simplified_chinese', 'lower_case', 'full_to_half']}, 'eval_config': {'batch_size': 64, 'model_file': 'model.pt'}, 'train_config': {'model_class': 'BERT_AVG', 'kd': {'use_kd': False, 'teacher_dir': '../output/post_sentiment_20210707_bert_avg/model', 'loss_type': 'mse', 'soft_lambda': 0.5, 'kl_T': 5}, 'seed': 42, 'log_steps': 100, 'batch_size': 32, 'final_model': 'best', 'optimization_metric': 'macro_f1', 'early_stop': 5}, 'device': 0, 'model_config': {'max_length': 256, 'tokenizer_source': 'transformers', 'tokenizer_name': 'bert-base-chinese', 'pretrained_lm': 'bert-base-chinese', 'output_hidden_act_func': 'ReLU', 'output_hidden_dim': 128, 'num_train_epochs': 10, 'optimizer': 'AdamW', 'learning_rate': '2e-5', 'weight_decay': 0.0, 'gradient_accumulation_steps': 1, 'adam_epsilon': '1e-8', 'max_grad_norm': 1.0, 'scheduler': 'linear', 'max_steps': -1, 'warmup_steps': 0, 'pretrained_lm_from_prev': '../config/examples/sequence_classification/BERT_AVG/model'}, 'label_to_id': {'1': 0, '0': 1, '-1': 2}, 'label_to_id_inv': {0: '1', 1: '0', 2: '-1'}, 'model_dir': PosixPath('../config/examples/sequence_classification/BERT_AVG/model')}\n"
     ]
    }
   ],
   "source": [
    "print('actual arguments: \\n', args.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get label ratio of train set\n",
    "\n",
    "This part should skip if user inputs label_to_ratio argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../config/examples/sequence_classification/BERT_AVG/model\n",
      "['run.yaml', 'model.yaml', 'tokenizer', 'label_to_id.json', 'model.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 62.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# if following ratio of train set\n",
    "from tokenizer import get_tokenizer\n",
    "from dataset import get_dataset\n",
    "    \n",
    "tokenizer = get_tokenizer(args = args) \n",
    "train_dataset = get_dataset(dataset=\"train\", tokenizer=tokenizer, args=args)\n",
    "\n",
    "print('train dataset size: ', len(train_dataset))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first three labels of trainset: \n",
      " [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "label = [train_dataset[i]['label'].item() for i in range(len(train_dataset))]\n",
    "print('The first three labels of trainset: \\n', label[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label ratio of train set: \n",
      " {'1': 0.3333333333333333, '0': 0.3333333333333333, '-1': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "def get_label_ratio(label):\n",
    "    '''\n",
    "        input:\n",
    "        - label: list\n",
    "\n",
    "        output:\n",
    "        - label_ratio: dict\n",
    "    '''\n",
    "    result = {}\n",
    "    for i in label:\n",
    "        key = args.label_to_id_inv[i]\n",
    "        if key not in result:\n",
    "            result[key] = 0\n",
    "        result[key] = result[key] + 1/len(label)\n",
    "    return result\n",
    "\n",
    "label_ratio = get_label_ratio(label)\n",
    "print('label ratio of train set: \\n',label_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate pseudo prediction label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 172.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlabel dataset size:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args.data_config['data_dir'] = '/'.join(unlabel_path.split('/')[:-1])\n",
    "args.data_dir = Path(args.data_config['data_dir'])\n",
    "args.data_config['unlabeled'] = unlabel_path.split('/')[-1]\n",
    "unlabel_dataset = get_dataset(dataset=\"unlabeled\", tokenizer=tokenizer, args=args)\n",
    "print('unlabel dataset size: ', len(unlabel_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import get_model\n",
    "model = get_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first three labels of unlabel: \n",
      " ['-1', '-1', '-1']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from trainer import prediction_step\n",
    "from itertools import chain\n",
    "\n",
    "def predict_label(dataset, model):\n",
    "    '''\n",
    "        input:\n",
    "        - dataset: torch.dataset\n",
    "        - model: torch.model\n",
    "        \n",
    "        output:\n",
    "        - list\n",
    "    '''\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=args.eval_config[\"batch_size\"],\n",
    "        # collate_fn=eval_dataset.pad_collate,\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for batch in dataloader:\n",
    "        result = prediction_step(model, batch, args=args)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "prediction = predict_label(unlabel_dataset, model)\n",
    "pseudo_label = prediction[0]['prediction']\n",
    "print('The first three labels of unlabel: \\n', pseudo_label[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### overview pseudo label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pseudo label ratio of unlabel set: \n",
      " {'-1': 0.984375, '0': 0.015625}\n"
     ]
    }
   ],
   "source": [
    "pseudo_label_id = prediction[0]['prediction_id']\n",
    "pseudo_label_ratio = get_label_ratio(pseudo_label_id)\n",
    "print('pseudo label ratio of unlabel set: \\n',pseudo_label_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of pseudo label (left column label, right column count): \n",
      " [['-1' '63']\n",
      " ['0' '1']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "(unique, counts) = np.unique(np.array(pseudo_label), return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "print('Frequency of pseudo label (left column label, right column count): \\n', frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 0.3333333333333333, '0': 0.3333333333333333, '-1': 0.3333333333333333}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sampling \n",
    "- label ratio\n",
    "- certainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def collect_probability(prediction):\n",
    "    # collect probability of prediction\n",
    "    prob_ls = []\n",
    "    for batch_pred in prediction:\n",
    "            prob_ls = prob_ls + batch_pred['probabilities']\n",
    "    prob_np = np.array(prob_ls)\n",
    "    return prob_np\n",
    "\n",
    "def get_sampled_idx(prob_np, label_ratio, certainty, sample_size):\n",
    "    # sample size for labels\n",
    "    ss_idx = []\n",
    "    label_collection = {}\n",
    "    remain_size = sample_size\n",
    "\n",
    "    for i, key in enumerate(label_ratio.keys()):\n",
    "        key_id = args.label_to_id[key]\n",
    "\n",
    "        # sample size computation\n",
    "        if i != len(label_ratio.keys()) - 1:\n",
    "            # sample size follows label ratio\n",
    "            key_size = int(sample_size * label_ratio[key])\n",
    "            remain_size = remain_size - key_size\n",
    "        else:\n",
    "            key_size = remain_size\n",
    "\n",
    "        # basic information of label data\n",
    "        print('label: ',key)\n",
    "        label_idx = np.argwhere((prob_np.argmax(axis=1)==key_id)).flatten()\n",
    "        print('unlabeled data size of label', key, ': ', label_idx.shape[0])\n",
    "\n",
    "        # certainty index\n",
    "        key_certain_idx = np.argwhere((prob_np.argmax(axis=1)==key_id) & (prob_np.max(axis=1)>=certainty)).flatten()\n",
    "        print('filtered data size of label', key, ' that certainty >= ',certainty,': ', key_certain_idx.shape[0])\n",
    "        print('required data size of label ',key,': ',key_size)\n",
    "\n",
    "        # warning if not able to sample enough data (filtered size is smaller than required size)\n",
    "        if key_size > key_certain_idx.shape[0]:\n",
    "            print('\\t(Warning: only sample ',key_certain_idx.shape[0], ' example(s) for label ',key,' because required size > filtered size)')\n",
    "            if key_size <= label_idx.shape[0]:\n",
    "                print('\\t(Suggested Certainty for label ', key,': ', prob_np.max(axis=1)[(prob_np.argmax(axis=1)==key_id)][-key_size], ')')\n",
    "            else:\n",
    "                print('\\t(Suggested Ratio for label ', key,': ', label_idx.shape[0]/sample_size,')')\n",
    "            key_size = key_certain_idx.shape[0]\n",
    "\n",
    "        # append sampled index to list\n",
    "        ss_idx = ss_idx + (random.sample(key_certain_idx.tolist(), key_size))\n",
    "        label_collection[key] = key_size\n",
    "        print('\\n')\n",
    "\n",
    "    # ss_idx = random.sample(ss_idx, sample_size)\n",
    "    print('Final sampled size: ', len(ss_idx))\n",
    "    print('Final label count: ',label_collection)\n",
    "    return ss_idx\n",
    "\n",
    "def extract_data(dir, idx):\n",
    "    # indexing unlabel data\n",
    "    with open(dir, 'rb') as outfile:\n",
    "        unlabel_data = np.array(json.load(outfile))\n",
    "    return unlabel_data[idx].tolist()\n",
    "\n",
    "def extract_logits(prop_np, idx):\n",
    "    return prop_np[idx].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  1\n",
      "unlabeled data size of label 1 :  0\n",
      "filtered data size of label 1  that certainty >=  0.36 :  0\n",
      "required data size of label  1 :  3\n",
      "\t(Warning: only sample  0  example(s) for label  1  because required size > filtered size)\n",
      "\t(Suggested Ratio for label  1 :  0.0 )\n",
      "\n",
      "\n",
      "label:  0\n",
      "unlabeled data size of label 0 :  1\n",
      "filtered data size of label 0  that certainty >=  0.36 :  0\n",
      "required data size of label  0 :  3\n",
      "\t(Warning: only sample  0  example(s) for label  0  because required size > filtered size)\n",
      "\t(Suggested Ratio for label  0 :  0.1 )\n",
      "\n",
      "\n",
      "label:  -1\n",
      "unlabeled data size of label -1 :  63\n",
      "filtered data size of label -1  that certainty >=  0.36 :  63\n",
      "required data size of label  -1 :  4\n",
      "\n",
      "\n",
      "Final sampled size:  4\n",
      "Final label count:  {'1': 0, '0': 0, '-1': 4}\n"
     ]
    }
   ],
   "source": [
    "def sampling(unlabel_path, prediction, label_ratio, certainty, sample_size):\n",
    "    '''\n",
    "        input:\n",
    "        - unlabel_dataset: torch.dataset\n",
    "        - pseudo_label: list\n",
    "        - label_ratio: dict\n",
    "        - certainty: float\n",
    "\n",
    "        output:\n",
    "        - list\n",
    "    '''\n",
    "    # collect probability of prediction\n",
    "    prob_np = collect_probability(prediction)\n",
    "\n",
    "    # get sampled index\n",
    "    idx = get_sampled_idx(prob_np, label_ratio, certainty, sample_size)\n",
    "\n",
    "    # indexing unlabel data\n",
    "    sampled_data = extract_data(unlabel_path, idx)\n",
    "\n",
    "    # indexing unlabel logits\n",
    "    sampled_logits = extract_logits(prob_np, idx)\n",
    "\n",
    "    return sampled_data, sampled_logits\n",
    "\n",
    "sampled_data, sampled_logits = sampling(\n",
    "    unlabel_path = unlabel_path, \n",
    "    prediction = prediction, \n",
    "    label_ratio = label_ratio, \n",
    "    certainty = certainty,\n",
    "    sample_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of sampled data:  4\n",
      "Overview of first sampled data {'content': '【#27人一月檢測樣本超3000# 他們的戰場，在實驗室】#周刊君與你共同戰疫# 武漢肺科醫院有這樣一個檢驗團隊，他們藏身實驗室內，每天與看不見的病毒打交道。團隊27名同志，已經連續晝夜奮戰長達一個多月，累計檢測的新冠病毒樣本數量超過3000多例。他們每批次的提取檢測，有10多個步驟，全程要高度集中精神，尤其最容易感染的標本處理及核酸提取階段，絲毫不能分神。詳戳↓ @央視財經 http://t.cn/A6hvao74\\n'} \n",
      "\n",
      "Total size of sampled logits:  4\n",
      "Overview of first sampled logits [0.3296697437763214, 0.23053191602230072, 0.43979835510253906]\n"
     ]
    }
   ],
   "source": [
    "print('Total size of sampled data: ', len(sampled_data))\n",
    "print('Overview of first sampled data', sampled_data[0],'\\n')\n",
    "print('Total size of sampled logits: ', len(sampled_logits))\n",
    "print('Overview of first sampled logits', sampled_logits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save dataset and logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(sample_data: list, save_path: str):\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(sample_data, outfile)\n",
    "\n",
    "def save_logit(sample_logits: list, save_path: str):\n",
    "    import pickle\n",
    "    with open(save_path, 'wb') as outfile:\n",
    "        pickle.dump(sample_logits, outfile)\n",
    "\n",
    "save_data_path = save_dir + save_data_file\n",
    "save_logit_path = save_dir + save_logit_file\n",
    "save_data(sampled_data, save_data_path)\n",
    "save_logit(sampled_logits, save_logit_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### review saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "{'content': '【#27人一月檢測樣本超3000# 他們的戰場，在實驗室】#周刊君與你共同戰疫# 武漢肺科醫院有這樣一個檢驗團隊，他們藏身實驗室內，每天與看不見的病毒打交道。團隊27名同志，已經連續晝夜奮戰長達一個多月，累計檢測的新冠病毒樣本數量超過3000多例。他們每批次的提取檢測，有10多個步驟，全程要高度集中精神，尤其最容易感染的標本處理及核酸提取階段，絲毫不能分神。詳戳↓ @央視財經 http://t.cn/A6hvao74\\n'}\n"
     ]
    }
   ],
   "source": [
    "with open(save_data_path, 'rb') as outfile:\n",
    "    result = json.load(outfile)\n",
    "print(len(result))\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove file\n",
    "\n",
    "This part removes saved files for cleaning direcotry, skip below if saving data\n",
    "- code 0: sucessful removal\n",
    "- code 256: failed removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(os.system(f\"rm {save_data_path}\"))\n",
    "print(os.system(f\"rm {save_logit_path}\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
