{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "From data augmentation experiment for Apple Care 2 dataset, highlighted factors can ensure the quality of unlabeled dataset which enhance the performance of model.\n",
    "\n",
    "Experiment details in [2022_01_13 biweekly discussion.pptx](https://jira.wisers.com:18090/download/attachments/82808396/2022_01_13%20biweekly%20discussion.pptx?version=1&modificationDate=1642063801000&api=v2)\n",
    "\n",
    "For code design please browse [Confluence Proposed Module](https://jira.wisers.com:18090/display/RES/Proposed+Module2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/developer/Users/hinova/canton-target-sentiment/src'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../src/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Generation\n",
    "\n",
    "Model is required to predict labels for unlabel data in order to sample data. This step can be skipped if user already got trained model which exists:\n",
    "- model directory\n",
    "    - run.yaml\n",
    "    - model.yaml\n",
    "    - label_to_id.json\n",
    "    - model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f\"python run.py --config_dir='../config/examples/sequence_classification/BERT_AVG'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User can sample label ratio based on the ratio of train set, or request desired label ratio. It depends on existence of argument (either input this argument or not):\n",
    "\n",
    "- label_ratio (if required desired label ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user can sample label ratio based on the ratio of train set, or request desired label ratio\n",
    "\n",
    "# label_ratio = {'-1': 0.3, '0': 0.4, '1': 0.3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model_dir = '../config/examples/sequence_classification/BERT_AVG/model' # model inference\n",
    "\n",
    "# unlabel data path\n",
    "unlabel_path = '../data/datasets/sample/sequence_classification/unlabeled_sample.json' \n",
    "\n",
    "# save path\n",
    "save_path = '../data/datasets/sample/sequence_classification'\n",
    "\n",
    "# sample size and certainty\n",
    "sample_size = 150 # required, integer and smaller than size of unlabeled data\n",
    "certainty = 0.8 # required, only select the data that max(p)>certainty, \n",
    "                # p is the predicted probabilities (0 - 1) over the label space\n",
    "                # default certainty is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from utils import load_yaml\n",
    "class arg():\n",
    "    def __init__(self, model_dir: str):\n",
    "        # run_yaml configuration\n",
    "        run_config = load_yaml(Path(model_dir) / \"run.yaml\")\n",
    "        self.task = run_config['task']\n",
    "        self.data_config = run_config['data']\n",
    "        self.data_dir = Path(self.data_config['data_dir'])\n",
    "        self.prepro_config = run_config['text_prepro']\n",
    "        self.eval_config = run_config['eval']\n",
    "        self.train_config = run_config['train']\n",
    "        self.device = run_config['device']\n",
    "        model_class = self.train_config['model_class']\n",
    "\n",
    "        # model_yaml configuration\n",
    "        self.model_config = load_yaml(Path(model_dir) / \"model.yaml\")[model_class]\n",
    "        self.model_config['pretrained_lm_from_prev'] = model_dir\n",
    "\n",
    "        # label_to_id mapping\n",
    "        with open(model_dir+'/label_to_id.json', 'rb') as outfile:\n",
    "            self.label_to_id = json.load(outfile)\n",
    "        self.label_to_id_inv = dict(zip(self.label_to_id.values(), self.label_to_id.keys()))\n",
    "\n",
    "        # model directory which model locates\n",
    "        self.model_dir = Path(model_dir)\n",
    "\n",
    "args = arg(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual arguments: \n",
      " {'task': 'sequence_classification', 'data_config': {'output_dir': '../config/examples/sequence_classification/BERT_AVG', 'data_dir': '../data/datasets/sample/sequence_classification', 'train': 'train_sample.json', 'dev': 'train_sample.json', 'test': 'train_sample.json'}, 'data_dir': PosixPath('../data/datasets/sample/sequence_classification'), 'prepro_config': {'steps': ['utf8_replace', 'simplified_chinese', 'lower_case', 'full_to_half']}, 'eval_config': {'batch_size': 64, 'model_file': 'model.pt'}, 'train_config': {'model_class': 'BERT_AVG', 'kd': {'use_kd': False, 'teacher_dir': '../output/post_sentiment_20210707_bert_avg/model', 'loss_type': 'mse', 'soft_lambda': 0.5, 'kl_T': 5}, 'seed': 42, 'log_steps': 100, 'batch_size': 32, 'final_model': 'best', 'optimization_metric': 'macro_f1', 'early_stop': 5}, 'device': 1, 'model_config': {'max_length': 256, 'tokenizer_source': 'transformers', 'tokenizer_name': 'bert-base-chinese', 'pretrained_lm': 'bert-base-chinese', 'output_hidden_act_func': 'ReLU', 'output_hidden_dim': 128, 'num_train_epochs': 10, 'optimizer': 'AdamW', 'learning_rate': '2e-5', 'weight_decay': 0.0, 'gradient_accumulation_steps': 1, 'adam_epsilon': '1e-8', 'max_grad_norm': 1.0, 'scheduler': 'linear', 'max_steps': -1, 'warmup_steps': 0, 'pretrained_lm_from_prev': '../config/examples/sequence_classification/BERT_AVG/model'}, 'label_to_id': {'1': 0, '0': 1, '-1': 2}, 'label_to_id_inv': {0: '1', 1: '0', 2: '-1'}, 'model_dir': PosixPath('../config/examples/sequence_classification/BERT_AVG/model')}\n"
     ]
    }
   ],
   "source": [
    "print('actual arguments: \\n', args.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get label ratio of train set\n",
    "\n",
    "This part should skip if user inputs label_to_ratio argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../config/examples/sequence_classification/BERT_AVG/model\n",
      "['run.yaml', 'model.yaml', 'tokenizer', 'label_to_id.json', 'model.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 104.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# if following ratio of train set\n",
    "from tokenizer import get_tokenizer\n",
    "from dataset import get_dataset\n",
    "    \n",
    "tokenizer = get_tokenizer(args = args) \n",
    "train_dataset = get_dataset(dataset=\"train\", tokenizer=tokenizer, args=args)\n",
    "\n",
    "print('train dataset size: ', len(train_dataset))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first three labels of trainset: \n",
      " [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "label = [train_dataset[i]['label'].item() for i in range(len(train_dataset))]\n",
    "print('The first three labels of trainset: \\n', label[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label ratio of train set: \n",
      " {'1': 0.3333333333333333, '0': 0.3333333333333333, '-1': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "def get_label_ratio(label):\n",
    "    '''\n",
    "        input:\n",
    "        - label: list\n",
    "\n",
    "        output:\n",
    "        - label_ratio: dict\n",
    "    '''\n",
    "    result = {}\n",
    "    for i in label:\n",
    "        key = args.label_to_id_inv[i]\n",
    "        if key not in result:\n",
    "            result[key] = 0\n",
    "        result[key] = result[key] + 1/len(label)\n",
    "    return result\n",
    "\n",
    "label_ratio = get_label_ratio(label)\n",
    "print('label ratio of train set: \\n',label_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate pseudo prediction label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 167.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlabel dataset size:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args.data_config['data_dir'] = '/'.join(unlabel_path.split('/')[:-1])\n",
    "args.data_dir = Path(args.data_config['data_dir'])\n",
    "args.data_config['unlabeled'] = unlabel_path.split('/')[-1]\n",
    "unlabel_dataset = get_dataset(dataset=\"unlabeled\", tokenizer=tokenizer, args=args)\n",
    "print('unlabel dataset size: ', len(unlabel_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import get_model\n",
    "model = get_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first three labels of unlabel: \n",
      " ['-1', '-1', '-1']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from trainer import prediction_step\n",
    "from itertools import chain\n",
    "\n",
    "def predict_label(dataset, model):\n",
    "    '''\n",
    "        input:\n",
    "        - dataset: torch.dataset\n",
    "        - model: torch.model\n",
    "        \n",
    "        output:\n",
    "        - list\n",
    "    '''\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=args.eval_config[\"batch_size\"],\n",
    "        # collate_fn=eval_dataset.pad_collate,\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for batch in dataloader:\n",
    "        result = prediction_step(model, batch, args=args)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "prediction = predict_label(unlabel_dataset, model)\n",
    "pseudo_label = prediction[0]['prediction']\n",
    "print('The first three labels of unlabel: \\n', pseudo_label[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '1', 1: '0', 2: '-1'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.label_to_id_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pseudo label ratio of unlabel set: \n",
      " {'-1': 0.8125, '1': 0.140625, '0': 0.046875}\n"
     ]
    }
   ],
   "source": [
    "pseudo_label_id = prediction[0]['prediction_id']\n",
    "pseudo_label_ratio = get_label_ratio(pseudo_label_id)\n",
    "print('pseudo label ratio of unlabel set: \\n',pseudo_label_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sampling \n",
    "- label ratio\n",
    "- certainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def sampling(unlabel_path, pseudo_label, label_ratio, certainty):\n",
    "    '''\n",
    "        input:\n",
    "        - unlabel_dataset: torch.dataset\n",
    "        - pseudo_label: list\n",
    "        - label_ratio: dict\n",
    "        - certainty: float\n",
    "\n",
    "        output:\n",
    "        - list\n",
    "    '''\n",
    "    # collect probability of prediction\n",
    "    prob_ls = []\n",
    "    for batch_pred in pseudo_label:\n",
    "            prob_ls = prob_ls + batch_pred['probabilities']\n",
    "    prob_np = np.array(prob_ls)\n",
    "\n",
    "    # sample size for labels\n",
    "    ss_idx = []\n",
    "    remain_size = sample_size\n",
    "    for i, key in enumerate(label_ratio.keys()):\n",
    "        # sample size computation\n",
    "        if i != len(label_ratio.keys()) - 1:\n",
    "            # sample size follows label ratio\n",
    "            key_size = int(sample_size * label_ratio[key])\n",
    "            remain_size = remain_size - key_size\n",
    "        else:\n",
    "            key_size = remain_size\n",
    "        # certainty index\n",
    "        key_certain_idx = np.argwhere((prob_np.argmax(axis=1)==key) & (prob_np.max(axis=1)>=certainty)).flatten()\n",
    "        ss_idx = ss_idx + (random.sample(key_certain_idx.tolist(), key_size))\n",
    "    ss_idx = random.sample(ss_idx, sample_size)\n",
    "\n",
    "    # indexing unlabel data\n",
    "    with open(unlabel_path, 'rb') as outfile:\n",
    "        unlabel_data = np.array(json.load(outfile))\n",
    "    return unlabel_data[ss_idx].tolist()\n",
    "\n",
    "sampled_ls = sampling(\n",
    "    unlabel_path = data_dir + '/' + unlabel_file, \n",
    "    pseudo_label = prediction, \n",
    "    label_ratio = label_ratio, \n",
    "    certainty = certainty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sampled label ratio output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "{'content': '【创意无限】这个iphone扩音器应该是你最好的朋友。是橡胶做的。它轻盈,柔软,可爱!更酷的是,它不需要电源驱动,只需将扬声器放在iphone的底部,就可以放大手机播放的声音。'}\n"
     ]
    }
   ],
   "source": [
    "print('Total size of sampled result', len(sampled_ls))\n",
    "print('Overview of ', sampled_ls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(sample_list: list, save_path: str):\n",
    "    with open(save_path, 'w') as outfile:\n",
    "        json.dump(sample_list, outfile)\n",
    "\n",
    "save_data(sampled_ls, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "{'content': '【创意无限】这个iphone扩音器应该是你最好的朋友。是橡胶做的。它轻盈,柔软,可爱!更酷的是,它不需要电源驱动,只需将扬声器放在iphone的底部,就可以放大手机播放的声音。'}\n"
     ]
    }
   ],
   "source": [
    "with open(save_path, 'rb') as outfile:\n",
    "    result = json.load(outfile)\n",
    "print(len(result))\n",
    "print(result[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
