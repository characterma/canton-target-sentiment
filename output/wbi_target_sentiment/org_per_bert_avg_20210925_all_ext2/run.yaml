task: "target_classification"
device: 0
data:
  output_dir: "../output/wbi_target_sentiment/org_per_bert_avg_20210925_all_ext2"
  data_dir: "../data/datasets/internal/target_sentiment/wbi_comb"
  train: train_ext.json
  dev: dev_ext.json
  test: test_ext.json
  unlabeled: unlabeled.json
  extra_tokens: nan.json
text_prepro:
  steps:
    - concat_headline_content_with_sep
    - concat_pub_code
    - concat_entity_name
    - extract_post_context_1
    - simplified_chinese
    - full_to_half
eval:
  batch_size: 16
  model_file: "model.pt"
train:
  model_class: "TDBERT"
  kd:
    use_kd: False
    teacher_dir: "../output/post_sentiment_20210707_bert_avg/model"
    loss_type: 'mse'
    soft_lambda: 0.5
    kl_T: 5
  seed: 42
  log_steps: 100
  batch_size: 16
  final_model: "best" # "best" or "last"
  optimization_metric: "macro_f1" # "micro_f1" or "macro_f1"
  early_stop: null
model_params:
  # ================= TDBERT =================
  max_length: 256
  num_train_epochs: 10
  embedding_trainable: True
  output_hidden_act_func: "PReLU"
  output_hidden_dim: 128
  output_use_bn: False
  attention_type: additive
  tokenizer_name: "hfl/chinese-macbert-base"  # voidful/albert_chinese_tiny
  pretrained_lm: "hfl/chinese-macbert-base" # voidful/albert_chinese_tiny
  # pretrained_lm_dir: "bert-base-chinese" # huggingface format "toastynews/electra-hongkongese-large-discriminator"
  # ================= TGSAN =================
  # num_train_epochs: 50
  # pretrained_emb_path: "../data/word_embeddings/sgns.target.word-character.char1-2.dynwin5.thr10.neg5.dim300.iter5"
  # embedding_trainable: False
explanation:
    Random:
      method: "Random" # Random LayerIntegratedGradients Custom
      batch_size: 1
      exclude_cls_sep: True
    AttnSum:
      method: "Custom" # Random LayerIntegratedGradients Custom
      model_output: "attentions"
      attn_agg_method: "ATTN_SUM"
      batch_size: 1
      exclude_cls_sep: True
    AttnBackprop: 
      method: "Custom" # Random LayerIntegratedGradients Custom
      model_output: "attentions"
      attn_agg_method: "ATTN_BACKPROP"
      batch_size: 1
      exclude_cls_sep: True
    IntegratedGradients:
      method: "LayerIntegratedGradients" # Random LayerIntegratedGradients Custom
      layer: "pretrained_model.embeddings.word_embeddings"
      batch_size: 1
      exclude_cls_sep: True
