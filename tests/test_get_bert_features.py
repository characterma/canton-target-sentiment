import unittest
import sys
import torch
from transformers import AutoTokenizer

sys.path.append("../src/")
from dataset import TargetDependentExample


class TestGetBertFeatures(unittest.TestCase):
    def test_get_bert_features(self):
        raw_text = "#仪式感不能少没有卡地亚，没有浪琴，但是我有阿玛尼，“我愿意把星辰银河都送给你”别说人间不值得 你最值得！"
        target_locs = [[15, 17]]
        tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese", use_fast=True)
        required_features = [
            "raw_text",
            "attention_mask",
            "token_type_ids",
            "target_mask",
            "label",
        ]

        feature_dict, msg = TargetDependentExample.get_bert_features(
            raw_text=raw_text,
            target_locs=target_locs,
            tokenizer=tokenizer,
            required_features=required_features,
            max_length=80,
            label="positive",
        )

        raw_text = torch.tensor(
            [
                101,
                108,
                811,
                2466,
                2697,
                679,
                5543,
                2208,
                3766,
                3300,
                1305,
                1765,
                762,
                8024,
                3766,
                3300,
                3857,
                4433,
                8024,
                852,
                3221,
                2769,
                3300,
                7350,
                4377,
                2225,
                8024,
                100,
                2769,
                2703,
                2692,
                2828,
                3215,
                6801,
                7213,
                3777,
                6963,
                6843,
                5314,
                872,
                100,
                1166,
                6432,
                782,
                7313,
                679,
                966,
                2533,
                872,
                3297,
                966,
                2533,
                8013,
                102,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
            ]
        )

        tokens = [
            "[CLS]",
            "#",
            "仪",
            "式",
            "感",
            "不",
            "能",
            "少",
            "没",
            "有",
            "卡",
            "地",
            "亚",
            "，",
            "没",
            "有",
            "浪",
            "琴",
            "，",
            "但",
            "是",
            "我",
            "有",
            "阿",
            "玛",
            "尼",
            "，",
            "[UNK]",
            "我",
            "愿",
            "意",
            "把",
            "星",
            "辰",
            "银",
            "河",
            "都",
            "送",
            "给",
            "你",
            "[UNK]",
            "别",
            "说",
            "人",
            "间",
            "不",
            "值",
            "得",
            "你",
            "最",
            "值",
            "得",
            "！",
            "[SEP]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
        ]

        target_tokens = ['浪', '琴']

        target_mask = torch.tensor(
            [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
            ]
        )

        attention_mask = torch.tensor(
            [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
            ]
        )

        token_type_ids = torch.tensor(
            [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
            ]
        )

        label = torch.tensor(2)

        self.assertTrue(msg == "")
        self.assertTrue(torch.equal(feature_dict["raw_text"], raw_text))
        self.assertTrue(feature_dict["tokens"]==tokens)
        self.assertTrue(feature_dict["target_tokens"]==target_tokens)
        self.assertTrue(torch.equal(feature_dict["target_mask"], target_mask))
        self.assertTrue(torch.equal(feature_dict["attention_mask"], attention_mask))
        self.assertTrue(torch.equal(feature_dict["token_type_ids"], token_type_ids))
        self.assertTrue(torch.equal(feature_dict["label"], label))
