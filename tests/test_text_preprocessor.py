import unittest
import sys

from nlp_pipeline.preprocess import Preprocessor
from collections import namedtuple


class TestPreprocessor(unittest.TestCase):
    def test_simplified_chinese(self):
        data_dict_1 = {
            'content': "#å„€å¼æ„Ÿä¸èƒ½å°‘æ²’æœ‰å¡åœ°äºï¼Œæ²’æœ‰æµªç´ï¼Œä½†æ˜¯æˆ‘æœ‰é˜¿ç‘ªå°¼ï¼Œâ€œæˆ‘é¡˜æ„æŠŠæ˜Ÿè¾°éŠ€æ²³éƒ½é€çµ¦ä½ â€åˆ¥èªªäººé–“ä¸å€¼å¾— ä½ æœ€å€¼å¾—ï¼", 
            'target_locs': [[15, 17]]
        }

        data_dict_2 = {
            'content': "#ä»ªå¼æ„Ÿä¸èƒ½å°‘æ²¡æœ‰å¡åœ°äºšï¼Œæ²¡æœ‰æµªç´ï¼Œä½†æ˜¯æˆ‘æœ‰é˜¿ç›å°¼ï¼Œâ€œæˆ‘æ„¿æ„æŠŠæ˜Ÿè¾°é“¶æ²³éƒ½é€ç»™ä½ â€åˆ«è¯´äººé—´ä¸å€¼å¾— ä½ æœ€å€¼å¾—ï¼", 
            'target_locs': [[15, 17]]
        }

        pp = Preprocessor(
            data_dict=data_dict_1, steps=["simplified_chinese"]
        )

        self.assertTrue(pp.data_dict['content'] == data_dict_2['content'])

    def rm_emojis(self):
        data_dict_1 = {
            'content': "#[é¼“æŒ]ä»ªå¼æ„Ÿä¸èƒ½å°‘æ²¡æœ‰å¡åœ°äºšï¼Œæ²¡æœ‰æµªç´ï¼Œä½†æ˜¯æˆ‘æœ‰é˜¿ç›å°¼ï¼Œâ€œæˆ‘æ„¿æ„æŠŠæ˜Ÿè¾°é“¶æ²³éƒ½é€ç»™ä½ â€åˆ«è¯´äººé—´ä¸å€¼å¾— ä½ æœ€å€¼å¾—ï¼[å¤ªå¼€å¿ƒ]", 
            'target_locs': [[19, 21]]
        }

        data_dict_2 = {
            'content': "#ä»ªå¼æ„Ÿä¸èƒ½å°‘æ²¡æœ‰å¡åœ°äºšï¼Œæ²¡æœ‰æµªç´ï¼Œä½†æ˜¯æˆ‘æœ‰é˜¿ç›å°¼ï¼Œâ€œæˆ‘æ„¿æ„æŠŠæ˜Ÿè¾°é“¶æ²³éƒ½é€ç»™ä½ â€åˆ«è¯´äººé—´ä¸å€¼å¾— ä½ æœ€å€¼å¾—ï¼", 
            'target_locs': [[15, 17]]
        }

        pp = Preprocessor(
            data_dict=data_dict_1, steps=["rm_emojis"]
        )

        self.assertTrue(pp.data_dict['content'] == data_dict_2['content'])
        self.assertTrue(pp.data_dict['target_locs'] == data_dict_2['target_locs'])


    def test_enclose_target(self):
        data_dict_1 = {
            'content': "#ä»ªå¼æ„Ÿä¸èƒ½å°‘æ²¡æœ‰å¡åœ°äºšï¼Œæ²¡æœ‰æµªç´ï¼Œä½†æ˜¯æˆ‘æœ‰é˜¿ç›å°¼ï¼Œâ€œæˆ‘æ„¿æ„æŠŠæ˜Ÿè¾°é“¶æ²³éƒ½é€ç»™ä½ â€åˆ«è¯´äººé—´ä¸å€¼å¾— ä½ æœ€å€¼å¾—ï¼", 
            'target_locs': [[15, 17]]
        }
        data_dict_2 = {
            'content': "#ä»ªå¼æ„Ÿä¸èƒ½å°‘æ²¡æœ‰å¡åœ°äºšï¼Œæ²¡æœ‰[E]æµªç´[/E]ï¼Œä½†æ˜¯æˆ‘æœ‰é˜¿ç›å°¼ï¼Œâ€œæˆ‘æ„¿æ„æŠŠæ˜Ÿè¾°é“¶æ²³éƒ½é€ç»™ä½ â€åˆ«è¯´äººé—´ä¸å€¼å¾— ä½ æœ€å€¼å¾—ï¼", 
            'target_locs': [[15, 18]]
        }
        pp = Preprocessor(
            data_dict=data_dict_1, steps=["enclose_target"]
        )

        self.assertTrue(pp.data_dict['content'] == data_dict_2['content'])
        self.assertTrue(pp.data_dict['target_locs'] == data_dict_2['target_locs'])

    def test_normalize_target(self):
        data_dict_1 = {
            'content': "#ä»ªå¼æ„Ÿä¸èƒ½å°‘æ²¡æœ‰å¡åœ°äºšï¼Œæ²¡æœ‰æµªç´ï¼Œä½†æ˜¯æˆ‘æœ‰é˜¿ç›å°¼ï¼Œâ€œæˆ‘æ„¿æ„æŠŠæ˜Ÿè¾°é“¶æ²³éƒ½é€ç»™ä½ â€åˆ«è¯´äººé—´ä¸å€¼å¾— ä½ æœ€å€¼å¾—ï¼", 
            'target_locs': [[15, 17]]
        }
        data_dict_2 = {
            'content': "#ä»ªå¼æ„Ÿä¸èƒ½å°‘æ²¡æœ‰å¡åœ°äºšï¼Œæ²¡æœ‰[unused1]ï¼Œä½†æ˜¯æˆ‘æœ‰é˜¿ç›å°¼ï¼Œâ€œæˆ‘æ„¿æ„æŠŠæ˜Ÿè¾°é“¶æ²³éƒ½é€ç»™ä½ â€åˆ«è¯´äººé—´ä¸å€¼å¾— ä½ æœ€å€¼å¾—ï¼", 
            'target_locs': [[15, 24]]
        }
        pp = Preprocessor(
            data_dict=data_dict_1, steps=["normalize_target"]
        )

        self.assertTrue(pp.data_dict['content'] == data_dict_2['content'])
        self.assertTrue(pp.data_dict['target_locs'] == data_dict_2['target_locs'])

    def test_replace_punctuation_with_space(self):
        data_dict_1 = {
            'content': "#ä»ªå¼æ„Ÿä¸èƒ½å°‘æ²¡æœ‰å¡åœ°äºšï¼Œæ²¡æœ‰æµªç´ï¼Œä½†æ˜¯æˆ‘æœ‰é˜¿ç›å°¼ï¼Œâ€œæˆ‘æ„¿æ„æŠŠæ˜Ÿè¾°é“¶æ²³éƒ½é€ç»™ä½ â€åˆ«è¯´äººé—´ä¸å€¼å¾— ä½ æœ€å€¼å¾—ï¼", 
            'target_locs': [[15, 17]]
        }
        data_dict_2 = {
            'content': " ä»ªå¼æ„Ÿä¸èƒ½å°‘æ²¡æœ‰å¡åœ°äºš æ²¡æœ‰æµªç´ ä½†æ˜¯æˆ‘æœ‰é˜¿ç›å°¼  æˆ‘æ„¿æ„æŠŠæ˜Ÿè¾°é“¶æ²³éƒ½é€ç»™ä½  åˆ«è¯´äººé—´ä¸å€¼å¾— ä½ æœ€å€¼å¾— ", 
            'target_locs': [[15, 17]]
        }
        pp = Preprocessor(
            data_dict=data_dict_1, steps=["replace_punctuation_with_space"]
        )

        self.assertTrue(pp.data_dict['content'] == data_dict_2['content'])
        self.assertTrue(pp.data_dict['target_locs'] == data_dict_2['target_locs'])

    def test_mask_other_targets(self):
        data_dict_1 = {
            'content': '2020å¹´åº•ä»¥æ¥,é»˜æ²™ä¸œã€ã€é˜¿æ–¯åˆ©åº·å’Œç½—æ°å‡çˆ†å‡ºåœ¨å›½å¤–æ’¤é”€PD-1L1äº§å“é€‚åº”ç—‡çš„æ¶ˆæ¯', 
            'target_locs': [[19, 21]], 
            'other_target_locs': [[0, 6], [9, 12], [14, 18], [19, 21]], 
        }
        data_dict_2 = {
            'content': '[unused2]ä»¥æ¥,[unused2]ã€ã€[unused2]å’Œç½—æ°å‡çˆ†å‡ºåœ¨å›½å¤–æ’¤é”€PD-1L1äº§å“é€‚åº”ç—‡çš„æ¶ˆæ¯', 
            'target_locs': [[33, 35]]
        }
        pp = Preprocessor(
            data_dict=data_dict_1, steps=["mask_other_targets"]
        )

        self.assertTrue(pp.data_dict['content'] == data_dict_2['content'])
        self.assertTrue(pp.data_dict['target_locs'] == data_dict_2['target_locs'])


    def test_lower_case(self):
        data_dict_1 = {
            'content': "Filled with jealousy, Omega made their thoughts known on social media."
        }
        data_dict_2 = {
            'content': "filled with jealousy, omega made their thoughts known on social media."
        }
        pp = Preprocessor(
            data_dict=data_dict_1, steps=["lower_case"]
        )
        self.assertTrue(pp.data_dict['content'] == data_dict_2['content'])

    def test_full_to_half(self):
        data_dict_1 = {'content': "ï¼ï¼Ÿï¼›ï¼Œ"}
        data_dict_2 = {'content': "!?;,"}
        pp = Preprocessor(
            data_dict=data_dict_1, steps=["full_to_half"]
        )
        self.assertEqual(pp.data_dict['content'], data_dict_2['content'])

    def test_rm_non_chinese_char(self):
        data_dict_1 = {'content': "#å¥½rrr_ç‰©_æ¨è–¦ğŸ”¥ ABC"}
        data_dict_2 = {'content': "å¥½ç‰©æ¨è–¦"}
        pp = Preprocessor(
            data_dict=data_dict_1, steps=["rm_non_chinese_char"]
        )
        self.assertEqual(pp.data_dict['content'], data_dict_2['content'])
        
    def test_convert_java_index(self):
        data_dict_1 = {
            'content': "#å¥½ç‰©æ¨è–¦ğŸ”¥ #æ¯æ—¥ç©¿æ­  å¡åœ°äºé‘°åŒ™ç³»åˆ— å–®è¡¨æ©Ÿæ¢°æ©ŸèŠ¯ 95â¤ æ»¿é‘½ è¶…å€¼ğŸ’°å¸¶èµ°#å¥½ç‰©æ¨è–¦ğŸ”¥ #æ¯æ—¥ç©¿æ­  å¡åœ°äºé‘°åŒ™ç³»åˆ— å–®è¡¨æ©Ÿæ¢°æ©ŸèŠ¯ 95â¤ æ»¿é‘½ è¶…å€¼ğŸ’°å¸¶èµ°", 
            'target_locs': [[15, 18], [25, 27], [27, 29], [58, 61], [68, 70], [70, 72]]    
        }
        data_dict_2 = {
            'content': "#å¥½ç‰©æ¨è–¦ğŸ”¥ #æ¯æ—¥ç©¿æ­  å¡åœ°äºé‘°åŒ™ç³»åˆ— å–®è¡¨æ©Ÿæ¢°æ©ŸèŠ¯ 95â¤ æ»¿é‘½ è¶…å€¼ğŸ’°å¸¶èµ°#å¥½ç‰©æ¨è–¦ğŸ”¥ #æ¯æ—¥ç©¿æ­  å¡åœ°äºé‘°åŒ™ç³»åˆ— å–®è¡¨æ©Ÿæ¢°æ©ŸèŠ¯ 95â¤ æ»¿é‘½ è¶…å€¼ğŸ’°å¸¶èµ°", 
            'target_locs': [[14, 17], [24, 26], [26, 28], [55, 58], [65, 67], [67, 69]]    
        }
        pp = Preprocessor(
            data_dict=data_dict_1, steps=["convert_java_index"]
        )
        self.assertTrue(pp.data_dict["content"] == data_dict_2["content"])
        self.assertTrue(pp.data_dict["target_locs"] == data_dict_2["target_locs"])
    
    def test_target_sentence_truncate(self):
        data_dict_1 = {
            'content':  "ç‚¹å‡»è“å­—å…³æ³¨æˆ‘ä»¬1. é’å²›ã€å¤©æ´¥ã€åŒ—äº¬ã€æ˜†æ˜ã€å¤§è¿æµ·å…³â€œé¾™è…¾â€è¡ŒåŠ¨æŸ¥åŠä¾µæƒç³»åˆ—æ¡ˆ2. å¤©æ´¥ã€å—äº¬ã€é»„åŸ”ã€ç¦å·ã€æ‹‰è¨æµ·å…³å¼€å±•ä¸­ä¿„æµ·å…³2018å¹´ä¸–ç•Œæ¯çŸ¥è¯†äº§æƒä¿æŠ¤è”åˆæ‰§æ³•è¡ŒåŠ¨ç³»åˆ—æ¡ˆ3. å¹¿å·ã€æ±Ÿé—¨ã€æ‹±åŒ—ã€æµ·å£æµ·å…³åœ¨å…¨å›½é€šå…³ä¸€ä½“åŒ–æ¨¡å¼ä¸‹åˆä½œæŸ¥è·ä¾µæƒè´§ç‰©ç³»åˆ—æ¡ˆ4. å—äº¬ã€é•¿æ²™ã€å—å®ã€é’å²›ã€çŸ³å®¶åº„ã€ä¹Œé²æœ¨é½æµ·å…³æŸ¥è·å‡ºå£è‡³ä¸€å¸¦ä¸€è·¯æ²¿çº¿å›½å®¶ä¾µæƒè´§ç‰©ç³»åˆ—æ¡ˆ5. é»„åŸ”æµ·å…³ã€æ‹±åŒ—æµ·å…³è·¨å¢ƒç”µå­å•†åŠ¡é›¶å”®è¿›å‡ºå£ä¸“é¡¹è¡ŒåŠ¨æŸ¥è·ä¾µæƒè´§ç‰©ç³»åˆ—æ¡ˆ6. ä¸Šæµ·æµ·å…³è¿›åšä¼šæœŸé—´æŸ¥åŠä¾µçŠ¯å‚å±•å•†çŸ¥è¯†äº§æƒè´§ç‰©ç³»åˆ—æ¡ˆ7. æ·±åœ³æµ·å…³æŸ¥è·å‡ºå£ä¾µçŠ¯â€œç©ºå¿ƒå¯¹ç®¡è½´é”®ç›˜â€ä¸“åˆ©æƒè´§ç‰©æ¡ˆ8. å¦é—¨æµ·å…³å»ºç«‹é£é™©æ¨¡å‹æŸ¥è·å‡ºå£ä¾µæƒé‹ç±»äº§å“ç³»åˆ—æ¡ˆ9. æ­å·æµ·å…³æŸ¥è·å‡ºå£å‡å†’â€œLVâ€å•†æ ‡å›´å·¾æ¡ˆ10.å®æ³¢æµ·å…³è¿›å£ç¯èŠ‚æŸ¥è·ä¾µçŠ¯â€œé«˜éœ²æ´â€å•†æ ‡ä¸“ç”¨æƒç‰™åˆ·æ¡ˆé’å²›ã€å¤©æ´¥ã€åŒ—äº¬ã€æ˜†æ˜ã€å¤§è¿æµ·å…³â€œé¾™è…¾â€è¡ŒåŠ¨æŸ¥åŠä¾µæƒç³»åˆ—æ¡ˆæµ·å…³æ€»ç½²äº2018å¹´8æœˆ1æ—¥è‡³11æœˆ30æ—¥éƒ¨ç½²å¼€å±•â€œé¾™è…¾â€è¡ŒåŠ¨(2018)", 
            'target_locs': [[327, 330]]    
        }
        data_dict_2 = {
            'content':  "ä¾µæƒè´§ç‰©ç³»åˆ—æ¡ˆ6. ä¸Šæµ·æµ·å…³è¿›åšä¼šæœŸé—´æŸ¥åŠä¾µçŠ¯å‚å±•å•†çŸ¥è¯†äº§æƒè´§ç‰©ç³»åˆ—æ¡ˆ7. æ·±åœ³æµ·å…³æŸ¥è·å‡ºå£ä¾µçŠ¯â€œç©ºå¿ƒå¯¹ç®¡è½´é”®ç›˜â€ä¸“åˆ©æƒè´§ç‰©æ¡ˆ8. å¦é—¨æµ·å…³å»ºç«‹é£é™©æ¨¡å‹æŸ¥è·å‡ºå£ä¾µæƒé‹ç±»äº§å“ç³»åˆ—æ¡ˆ9. æ­å·æµ·å…³æŸ¥è·å‡ºå£å‡å†’â€œLVâ€å•†æ ‡å›´å·¾æ¡ˆ10.å®æ³¢æµ·å…³è¿›å£ç¯èŠ‚æŸ¥è·ä¾µçŠ¯â€œé«˜éœ²æ´â€å•†æ ‡ä¸“ç”¨æƒç‰™åˆ·æ¡ˆé’å²›ã€å¤©æ´¥ã€åŒ—äº¬ã€æ˜†æ˜ã€å¤§è¿æµ·å…³â€œé¾™è…¾â€è¡ŒåŠ¨æŸ¥åŠä¾µæƒç³»åˆ—æ¡ˆæµ·å…³æ€»ç½²äº2018å¹´8æœˆ1æ—¥è‡³11æœˆ30æ—¥éƒ¨ç½²å¼€å±•â€œé¾™è…¾â€è¡ŒåŠ¨(2018)", 
            'target_locs': [[127, 130]]    
        }
        args = namedtuple('args', 'model_config')
        args.model_config = {'max_length': 256}
        pp = Preprocessor(
            data_dict=data_dict_1, steps=["target_sentence_truncate"], args=args
        )
        self.assertTrue(pp.data_dict["content"] == data_dict_2["content"])
        self.assertTrue(pp.data_dict["target_locs"] == data_dict_2["target_locs"])

    # def test_extract_post_context_1(self):
    #     data_dict_1 = {
    #         'content': "å‰å¹´8â€§18åä¿®ä¾‹ã€Œæµæ°´å¼é›†æœƒã€æœªç¶“æ‰¹å‡†é›†çµæ¡ˆä¸­ï¼Œç¾æ­£åœ¨èµ¤æŸ±ç›£ç„é‚„æŠ¼çš„å£¹å‚³åª’ã€‚é»æ™ºè‹±èˆ‡å¦å¤–8åæ³›æ°‘äººå£«è¢«æª¢æ§ï¼Œæ¡ˆä»¶å°‡æ–¼æ˜å¤©ï¼ˆ16æ—¥ï¼‰é–‹å¯©ï¼Œé è¨ˆå¯©æœŸ10å¤©ã€‚å…¶ä¸­ä¸€åè¢«å‘Šå€è«¾è»’æ—©å‰è¡¨æ˜æœƒèªç½ªï¼Œæ³•åº­æ˜å¤©æœƒæ­£å¼è½å–ä»–çš„èªç½ªç­”è¾¯ï¼Œè€Œå…¶é¤˜8åè¢«å‘Šå‡å·²å¦èªæ§ç½ªï¼Œç•¶ä¸­åŒ…æ‹¬é»æ™ºè‹±ã€‚ 9åè¢«å‘Šï¼ŒåŒ…æ‹¬é»æ™ºè‹±ã€ææŸ±éŠ˜ã€ä½•ä¿Šä»ã€æå“äººã€æ¢åœ‹é›„ã€å³é„å„€ã€æ¢è€€å¿ ã€ä½•ç§€è˜­åŠå€è«¾è»’ã€‚ä»–å€‘è¢«æ§ä¸€é …çµ„ç¹”æœªç¶“æ‰¹å‡†é›†çµåŠä¸€é …åƒèˆ‡æœªç¶“æ‰¹å‡†é›†çµç½ªã€‚å¾‹æ”¿å¸çš„æª¢æ§åœ˜éšŠæœƒç”±è³‡æ·±å¤§å¾‹å¸«ä½™è‹¥æµ·é ˜è»ï¼Œè€Œä»£è¡¨é»æ™ºè‹±çš„æ˜¯è³‡æ·±å¤§å¾‹å¸«ä½™è‹¥è–‡ï¼Œé€™å ´å®˜å¸å°‡ç”±å§Šå¼Ÿå°å£˜ã€‚ æ³•åº­æ–‡ä»¶é¡¯ç¤ºï¼Œæ§æ–¹æš«å®šæœ‰7åè­‰äººã€‚åŒ…æ‹¬6åè­¦å‹™äººå“¡å’Œä¸€åæ¸¯éµç¶“ç†ï¼Œä¸¦æœƒæœ‰35æ®µæ–°èå ±é“å’Œ27æ®µæ¡ˆç™¼ç‰‡æ®µå‘ˆå ‚ã€‚è¾¯æ–¹å°‡æœƒæå‡ºæ³•å¾‹çˆ­è­°ï¼Œè²ç¨±ã€Šå…¬å®‰æ¢ä¾‹ã€‹å°‡çµ„ç¹”æˆ–åƒèˆ‡å’Œå¹³é›†çµå®šç‚ºç½ªè¡Œï¼Œä¸¦è³¦æ¬Šè­¦å‹™è™•è™•é•·åå°é›†çµï¼Œå±¬æå®³å…¬æ°‘é›†æœƒæ¬Šåˆ©ã€‚å¦å¤–ï¼Œåœ¨æœ¬æ¡ˆæ‰€æ¶‰çš„é›†çµé—œä¹æ‰¹è©•è­¦éšŠï¼Œè¾¯æ–¹è³ªç–‘ç”±è­¦å‹™è™•è™•é•·æ±ºå®šæ˜¯å¦æ‰¹å‡†ï¼Œå±¬æœ‰åˆ©ç›Šè¡çªå’Œåé —ä¹‹å«Œã€‚æ­¤å¤–ï¼Œè¾¯æ–¹äº¦æœƒè³ªç–‘å…¬çœ¾é›†æœƒåŠéŠè¡Œä¸Šè¨´å§”å“¡æœƒæ˜¯å¦ç¨ç«‹å…¬æ­£ã€‚ OA_show(ONCC.Function.GetBanner.WriteOpenXAdZone('content_advContent',ONCC.Function.getSection(),ONCC.Function.getLocation(),ONCC.Function.getNation())); ", 
    #         'target_locs': [[82, 85], [171, 174]]
    #     }
    #     data_dict_2 = {
    #         'content': "å…¶ä¸­ä¸€åè¢«å‘Šå€è«¾è»’æ—©å‰è¡¨æ˜æœƒèªç½ªï¼Œæ³•åº­æ˜å¤©æœƒæ­£å¼è½å–ä»–çš„èªç½ªç­”è¾¯ï¼Œè€Œå…¶é¤˜8åè¢«å‘Šå‡å·²å¦èªæ§ç½ªï¼Œç•¶ä¸­åŒ…æ‹¬é»æ™ºè‹±ã€‚ 9åè¢«å‘Šï¼ŒåŒ…æ‹¬é»æ™ºè‹±ã€ææŸ±éŠ˜ã€ä½•ä¿Šä»ã€æå“äººã€æ¢åœ‹é›„ã€å³é„å„€ã€æ¢è€€å¿ ã€ä½•ç§€è˜­åŠå€è«¾è»’", 
    #         'target_locs': [[6, 9], [95, 98]]
    #     }
    #     pp = Preprocessor(
    #         data_dict=data_dict_1, steps=["extract_post_context_1"]
    #     )

    #     self.assertTrue(pp.data_dict['content'] == data_dict_2['content'])
    #     self.assertTrue(pp.data_dict['target_locs'] == data_dict_2['target_locs'])

    # def test_extract_post_context_2(self):
    #     data_dict_1 = {
    #         'content': "å‰å¹´8â€§18åä¿®ä¾‹ã€Œæµæ°´å¼é›†æœƒã€æœªç¶“æ‰¹å‡†é›†çµæ¡ˆä¸­ï¼Œç¾æ­£åœ¨èµ¤æŸ±ç›£ç„é‚„æŠ¼çš„å£¹å‚³åª’ã€‚é»æ™ºè‹±èˆ‡å¦å¤–8åæ³›æ°‘äººå£«è¢«æª¢æ§ï¼Œæ¡ˆä»¶å°‡æ–¼æ˜å¤©ï¼ˆ16æ—¥ï¼‰é–‹å¯©ï¼Œé è¨ˆå¯©æœŸ10å¤©ã€‚å…¶ä¸­ä¸€åè¢«å‘Šå€è«¾è»’æ—©å‰è¡¨æ˜æœƒèªç½ªï¼Œæ³•åº­æ˜å¤©æœƒæ­£å¼è½å–ä»–çš„èªç½ªç­”è¾¯ï¼Œè€Œå…¶é¤˜8åè¢«å‘Šå‡å·²å¦èªæ§ç½ªï¼Œç•¶ä¸­åŒ…æ‹¬é»æ™ºè‹±ã€‚ 9åè¢«å‘Šï¼ŒåŒ…æ‹¬é»æ™ºè‹±ã€ææŸ±éŠ˜ã€ä½•ä¿Šä»ã€æå“äººã€æ¢åœ‹é›„ã€å³é„å„€ã€æ¢è€€å¿ ã€ä½•ç§€è˜­åŠå€è«¾è»’ã€‚ä»–å€‘è¢«æ§ä¸€é …çµ„ç¹”æœªç¶“æ‰¹å‡†é›†çµåŠä¸€é …åƒèˆ‡æœªç¶“æ‰¹å‡†é›†çµç½ªã€‚å¾‹æ”¿å¸çš„æª¢æ§åœ˜éšŠæœƒç”±è³‡æ·±å¤§å¾‹å¸«ä½™è‹¥æµ·é ˜è»ï¼Œè€Œä»£è¡¨é»æ™ºè‹±çš„æ˜¯è³‡æ·±å¤§å¾‹å¸«ä½™è‹¥è–‡ï¼Œé€™å ´å®˜å¸å°‡ç”±å§Šå¼Ÿå°å£˜ã€‚ æ³•åº­æ–‡ä»¶é¡¯ç¤ºï¼Œæ§æ–¹æš«å®šæœ‰7åè­‰äººã€‚åŒ…æ‹¬6åè­¦å‹™äººå“¡å’Œä¸€åæ¸¯éµç¶“ç†ï¼Œä¸¦æœƒæœ‰35æ®µæ–°èå ±é“å’Œ27æ®µæ¡ˆç™¼ç‰‡æ®µå‘ˆå ‚ã€‚è¾¯æ–¹å°‡æœƒæå‡ºæ³•å¾‹çˆ­è­°ï¼Œè²ç¨±ã€Šå…¬å®‰æ¢ä¾‹ã€‹å°‡çµ„ç¹”æˆ–åƒèˆ‡å’Œå¹³é›†çµå®šç‚ºç½ªè¡Œï¼Œä¸¦è³¦æ¬Šè­¦å‹™è™•è™•é•·åå°é›†çµï¼Œå±¬æå®³å…¬æ°‘é›†æœƒæ¬Šåˆ©ã€‚å¦å¤–ï¼Œåœ¨æœ¬æ¡ˆæ‰€æ¶‰çš„é›†çµé—œä¹æ‰¹è©•è­¦éšŠï¼Œè¾¯æ–¹è³ªç–‘ç”±è­¦å‹™è™•è™•é•·æ±ºå®šæ˜¯å¦æ‰¹å‡†ï¼Œå±¬æœ‰åˆ©ç›Šè¡çªå’Œåé —ä¹‹å«Œã€‚æ­¤å¤–ï¼Œè¾¯æ–¹äº¦æœƒè³ªç–‘å…¬çœ¾é›†æœƒåŠéŠè¡Œä¸Šè¨´å§”å“¡æœƒæ˜¯å¦ç¨ç«‹å…¬æ­£ã€‚ OA_show(ONCC.Function.GetBanner.WriteOpenXAdZone('content_advContent',ONCC.Function.getSection(),ONCC.Function.getLocation(),ONCC.Function.getNation())); ", 
    #         'target_locs': [[82, 85], [171, 174]]
    #     }
    #     data_dict_2 = {
    #         'content': "é»æ™ºè‹±èˆ‡å¦å¤–8åæ³›æ°‘äººå£«è¢«æª¢æ§ï¼Œæ¡ˆä»¶å°‡æ–¼æ˜å¤©ï¼ˆ16æ—¥ï¼‰é–‹å¯©ï¼Œé è¨ˆå¯©æœŸ10å¤©ã€‚å…¶ä¸­ä¸€åè¢«å‘Šå€è«¾è»’æ—©å‰è¡¨æ˜æœƒèªç½ªï¼Œæ³•åº­æ˜å¤©æœƒæ­£å¼è½å–ä»–çš„èªç½ªç­”è¾¯ï¼Œè€Œå…¶é¤˜8åè¢«å‘Šå‡å·²å¦èªæ§ç½ªï¼Œç•¶ä¸­åŒ…æ‹¬é»æ™ºè‹±ã€‚ 9åè¢«å‘Šï¼ŒåŒ…æ‹¬é»æ™ºè‹±ã€ææŸ±éŠ˜ã€ä½•ä¿Šä»ã€æå“äººã€æ¢åœ‹é›„ã€å³é„å„€ã€æ¢è€€å¿ ã€ä½•ç§€è˜­åŠå€è«¾è»’ã€‚ä»–å€‘è¢«æ§ä¸€é …çµ„ç¹”æœªç¶“æ‰¹å‡†é›†çµåŠä¸€é …åƒèˆ‡æœªç¶“æ‰¹å‡†é›†çµç½ª", 
    #         'target_locs': [[44, 47], [133, 136]]
    #     }
    #     pp = Preprocessor(
    #         data_dict=data_dict_1, steps=["extract_post_context_2"]
    #     )
    #     self.assertTrue(pp.data_dict['content'] == data_dict_2['content'])
    #     self.assertTrue(pp.data_dict['target_locs'] == data_dict_2['target_locs'])
