import unittest
import sys
import torch
from transformers import AutoTokenizer

sys.path.append("../src/")
from dataset import TargetDependentExample


class TestGetFeatures(unittest.TestCase):
    def test_get_features(self):
        raw_text = "#‰ª™ÂºèÊÑü‰∏çËÉΩÂ∞ëÊ≤°ÊúâÂç°Âú∞‰∫öÔºå üî•Êµ™Áê¥Ôºå‰ΩÜÊòØÊàëÊúâÈòøÁéõÂ∞ºÔºå‚ÄúÊàëÊÑøÊÑèÊääÊòüËæ∞Èì∂Ê≤≥ÈÉΩÈÄÅÁªô‰Ω†‚ÄùÂà´ËØ¥‰∫∫Èó¥‰∏çÂÄºÂæó ‰Ω†ÊúÄÂÄºÂæóÔºÅ"
        target_locs = [[15, 17]]
        tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese", use_fast=True)
        required_features = [
            "raw_text",
            "attention_mask",
            "token_type_ids",
            "target_mask",
            "label",
        ]

        feature_dict, msg = TargetDependentExample.get_features(
            raw_text=raw_text,
            target_char_loc=target_locs,
            tokenizer=tokenizer,
            required_features=required_features,
            max_length=80,
            label="positive",
            label_to_id={"neutral": 0, "negative": 1, "positive": 2},
        )
        raw_text = torch.tensor(
            [
                101,
                108,
                811,
                2466,
                2697,
                679,
                5543,
                2208,
                3766,
                3300,
                1305,
                1765,
                762,
                8024,
                8103,
                3857,
                4433,
                8024,
                852,
                3221,
                2769,
                3300,
                7350,
                4377,
                2225,
                8024,
                100,
                2769,
                2703,
                2692,
                2828,
                3215,
                6801,
                7213,
                3777,
                6963,
                6843,
                5314,
                872,
                100,
                1166,
                6432,
                782,
                7313,
                679,
                966,
                2533,
                872,
                3297,
                966,
                2533,
                8013,
                102,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
            ]
        )

        tokens = [
            "[CLS]",
            "#",
            "‰ª™",
            "Âºè",
            "ÊÑü",
            "‰∏ç",
            "ËÉΩ",
            "Â∞ë",
            "Ê≤°",
            "Êúâ",
            "Âç°",
            "Âú∞",
            "‰∫ö",
            "Ôºå",
            "üî•",
            "Êµ™",
            "Áê¥",
            "Ôºå",
            "‰ΩÜ",
            "ÊòØ",
            "Êàë",
            "Êúâ",
            "Èòø",
            "Áéõ",
            "Â∞º",
            "Ôºå",
            "[UNK]",
            "Êàë",
            "ÊÑø",
            "ÊÑè",
            "Êää",
            "Êòü",
            "Ëæ∞",
            "Èì∂",
            "Ê≤≥",
            "ÈÉΩ",
            "ÈÄÅ",
            "Áªô",
            "‰Ω†",
            "[UNK]",
            "Âà´",
            "ËØ¥",
            "‰∫∫",
            "Èó¥",
            "‰∏ç",
            "ÂÄº",
            "Âæó",
            "‰Ω†",
            "ÊúÄ",
            "ÂÄº",
            "Âæó",
            "ÔºÅ",
            "[SEP]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
            "[PAD]",
        ]
        target_tokens = ["Êµ™", "Áê¥"]

        target_mask = torch.tensor(
            [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
            ]
        )

        attention_mask = torch.tensor(
            [
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
            ]
        )
        token_type_ids = torch.tensor(
            [
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                1,
                1,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
            ]
        )
        label = torch.tensor(2)
        self.assertTrue(torch.equal(feature_dict["raw_text"], raw_text))
        self.assertTrue(torch.equal(feature_dict["target_mask"], target_mask))
        self.assertTrue(torch.equal(feature_dict["attention_mask"], attention_mask))
        self.assertTrue(torch.equal(feature_dict["token_type_ids"], token_type_ids))
        self.assertTrue(torch.equal(feature_dict["label"], label))
