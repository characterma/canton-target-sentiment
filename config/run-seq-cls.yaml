task: "sequence_classification"
device: 0
data:
  output_dir: "../output/apple_care/in_out_domain_model_6/"
  data_dir: "../../sentiment_apple/data/apple_care/"
  train: hard_combine_3.json
  dev: sentiment_apple_dev_0.json
  test: sentiment_1.0_test.json
text_prepro:
  steps:
    - utf8_replace
    - simplified_chinese
    - lower_case
    - full_to_half
eval:
  batch_size: 64
  model_file: "model.pt"
train:
  model_class: "BERT_CLS_PRED"
  kd:
    use_kd: False
    teacher_dir: "../output/post_sentiment_20210707_bert_avg/model"
    loss_type: 'mse'
    soft_lambda: 0.5
    kl_T: 5
  seed: 42
  log_steps: 100
  batch_size: 16
  final_model: "best" # "best" or "last"
  optimization_metric: "macro_f1" # "micro_f1" or "macro_f1"
  early_stop: 5
model_params:
  # ================= TDBERT =================
  num_train_epochs: 10
  learning_rate: 3e-5
  embedding_trainable: True
  output_hidden_act_func: "PReLU"
  output_hidden_dim: 128
  # output_hidden_act_func: null
  tokenizer_name: "hfl/chinese-roberta-wwm-ext"
  pretrained_lm: "hfl/chinese-roberta-wwm-ext"
