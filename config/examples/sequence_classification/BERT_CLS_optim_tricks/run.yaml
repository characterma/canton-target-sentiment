task: "sequence_classification"
device: 0
data:
  output_dir: "../config/examples/sequence_classification/BERT_CLS_optim_tricks"
  data_dir: "../data/datasets/sample/sequence_classification"
  train: train_sample.json
  dev: train_sample.json
  test: train_sample.json
  unlabeled: unlabeled.json
  extra_special_tokens: 
    - unused5
  pad_in_collate: False
text_prepro:
  steps:
    - simplified_chinese
    - full_to_half
eval:
  batch_size: 16
  model_file: "model.pt"
train:
  model_class: "BERT_CLS"
  kd:
    use_kd: False
    teacher_dir: "../output/post_sentiment_20210707_bert_avg/model"
    loss_type: 'mse'
    soft_lambda: 0.5
    kl_T: 5
  seed: 42
  log_steps: 100
  batch_size: 16
  final_model: "best" # "best" or "last"
  optimization_metric: "macro_f1" # "micro_f1" or "macro_f1"
  early_stop: null
  r_drop_factor: 0.5
  enable_focal_loss: True
  focal_loss_gamma: 2
  focal_loss_reduction: mean
  enable_model_ema: False
  model_ema_alpha: 0.5
  model_ema_steps: 100
  enable_adversarial: True  # False
  adversarial_class: 'PGD' # FGM PGD
  adversarial_k: 3
  adversarial_param_names: [ 'pretrained_model.embeddings.']
  adversarial_alpha: 0.5
  adversarial_epsilon: 0.1
model_params:
  # ================= TDBERT =================
  max_length: 256
  num_train_epochs: 6
  embedding_trainable: True
  output_hidden_dim: 128
  output_use_bn: False
  tokenizer_name: "bert-base-chinese"  # source:::tokenizer_type "huggingface:::toastynews/electra-hongkongese-large-discriminator"
  pretrained_lm: "bert-base-chinese" # huggingface format "toastynews/electra-hongkongese-large-discriminator"
  special_token_path: null
explanation:
    Random:
      method: "Random"