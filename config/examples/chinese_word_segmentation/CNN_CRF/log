***** Args *****
   device: 0
   data: {'output_dir': '../config/examples/chinese_word_segmentation/CNN_CRF', 'data_dir': '../data/datasets/canton_ws', 'train': 'data_combined.json', 'dev': 'data_combined.json', 'test': 'data_combined.json'}
   text_prepro: {'steps': ['full_to_half']}
   eval: {'batch_size': 64, 'model_file': 'model.pt'}
   train: {'task': 'chinese_word_segmentation', 'model_class': 'BERT_CRF', 'seed': 42, 'batch_size': 16, 'final_model': 'best'}
   model_params: {'num_train_epochs': 10, 'embedding_trainable': True}
***** Loading tokenizer *****
  Tokenizer source = 'transformers'
  Tokenizer name = 'clue/albert_chinese_tiny'
***** Initializing model *****
  Task = chinese_word_segmentation
  Model class = BERT_CRF
***** Loading data *****
  Data path = ../data/datasets/canton_ws/data_combined.json
  Number of raw samples = 2445
  Number of loaded samples = 2445
***** Loading data *****
  Data path = ../data/datasets/canton_ws/data_combined.json
  Number of raw samples = 2445
  Number of loaded samples = 2445
***** Running training *****
  Num examples = 2445
  Num Epochs = 10
  Sampler = random
  Batch size = 16
  Gradient Accumulation steps = 1
