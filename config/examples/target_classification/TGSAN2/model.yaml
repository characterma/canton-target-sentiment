TDBERT:
    # ================= Preprocessing params =================
    max_length: 210
    tokenizer_source: "transformers"
    tokenizer_name: "clue/albert_chinese_tiny"  # source:::tokenizer_type "huggingface:::toastynews/electra-hongkongese-large-discriminator"   
    # ================= Model params =================
    pretrained_lm: "clue/albert_chinese_tiny" # huggingface format "toastynews/electra-hongkongese-large-discriminator" 
    dropout_rate: 0.1
    optimizer: "AdamW"
    sampler: "random"
    batch_size: 16
    learning_rate: 2e-5
    # ================= Training params =================
    num_train_epochs: 10
    weight_decay: 0.0
    gradient_accumulation_steps: 1
    adam_epsilon: 1e-8
    max_grad_norm: 1.0
    scheduler: "linear"
    max_steps: -1
    warmup_steps: 0
TGSAN:
    # ================= Preprocessing params =================
    max_length: 150
    tokenizer_source: "internal"
    tokenizer_name: "" 
    vocab_freq_cutoff: 0.1
    # ================= Model params =================
    pretrained_emb_path: null
    embedding_trainable: True
    emb_dim: 150
    emb_dropout: 0.5
    rnn_hidden_dim: 150
    r: 2
    tgt_san_dim: 64
    san_dropout: 0.1
    san_penal_coeff: 0.1
    ffn_dim: 16
    ffn_dropout: 0.1
    att_dropout: 0.1
    # ================= Training params =================
    num_train_epochs: 40
    batch_size: 64
    optimizer: "AdamW"
    learning_rate: 0.0015
    weight_decay: 0.9
    gradient_accumulation_steps: 1
    adam_epsilon: 1e-8
    max_grad_norm: 1.0
    max_steps: -1
    warmup_steps: 0
TGSAN2:
    # ================= Preprocessing params =================
    max_length: 256
    tokenizer_source: "transformers"
    tokenizer_name: "clue/albert_chinese_tiny"  # source:::tokenizer_type "huggingface:::toastynews/electra-hongkongese-large-discriminator"   
    # ================= Model params =================
    pretrained_lm: "clue/albert_chinese_tiny" # huggingface format "toastynews/electra-hongkongese-large-discriminator" 
    emb_dim: 300
    emb_dropout: 0.1
    encoder_dropout: 0.1
    encoder_activation: 'relu'
    fc_activation: 'ReLU'
    fc_dropout: 0.1
    n_encoder: 3
    optimizer: "AdamW"
    learning_rate: 2e-5
    # ================= Training params =================
    num_train_epochs: 10
    weight_decay: 0.0
    gradient_accumulation_steps: 1
    adam_epsilon: 1e-8
    max_grad_norm: 1.0
    scheduler: "linear"
    max_steps: -1
    warmup_steps: 0