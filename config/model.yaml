TDBERT:
  # ================= Preprocessing params =================
  max_length: 210
  tokenizer_source: "transformers"
  tokenizer_name: "bert-base-chinese"  # source:::tokenizer_type "huggingface:::toastynews/electra-hongkongese-large-discriminator"
  # ================= Model params =================
  pretrained_lm: "bert-base-chinese" # huggingface format "toastynews/electra-hongkongese-large-discriminator"
  dropout_rate: 0.1
  optimizer: "AdamW"
  batch_size: 16
  learning_rate: 2e-5
  output_hidden_dim: 128
  output_hidden_act_func: Tanh
#   # ================= Training params =================
  num_train_epochs: 10
  weight_decay: 0.0
  gradient_accumulation_steps: 1
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  max_steps: -1
  warmup_steps: 0
TGSAN:
  # ================= Preprocessing params =================
  max_length: 150
  tokenizer_source: "internal"
  tokenizer_name: ""
  vocab_freq_cutoff: 0.1
  # ================= Model params =================
  pretrained_emb_path: null
  embedding_trainable: True
  emb_dim: 150
  emb_dropout: 0.5
  rnn_hidden_dim: 150
  r: 2
  tgt_san_dim: 64
  san_dropout: 0.1
  san_penal_coeff: 0.1
  ffn_dim: 16
  ffn_dropout: 0.1
  att_dropout: 0.1
  # ================= Training params =================
  num_train_epochs: 40
  batch_size: 64
  optimizer: "AdamW"
  learning_rate: 0.0015
  weight_decay: 0.9
  gradient_accumulation_steps: 1
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  max_steps: -1
  warmup_steps: 0
TGSAN2:
    # ================= Preprocessing params =================
    max_length: 256
    tokenizer_source: "transformers"
    tokenizer_name: "clue/albert_chinese_tiny"  # source:::tokenizer_type "huggingface:::toastynews/electra-hongkongese-large-discriminator"
    # ================= Model params =================
    pretrained_lm: "clue/albert_chinese_tiny" # huggingface format "toastynews/electra-hongkongese-large-discriminator"
    emb_dim: 300
    emb_dropout: 0.1
    encoder_dropout: 0.1
    encoder_activation: 'relu'
    fc_activation: 'ReLU'
    fc_dropout: 0.1
    n_encoder: 3
    optimizer: "AdamW"
    learning_rate: 2e-5
    # ================= Training params =================
    num_train_epochs: 10
    weight_decay: 0.0
    gradient_accumulation_steps: 1
    adam_epsilon: 1e-8
    max_grad_norm: 1.0
    scheduler: "linear"
    max_steps: -1
    warmup_steps: 0
CNN_CRF:
  # ================= Preprocessing params =================
  max_length: 256
  tokenizer_source: "char_split"
  tokenizer_name: ""
  vocab_freq_cutoff: 0.01
  # ================= Model params =================
  pretrained_emb_path: null
  embedding_trainable: True
  emb_dim: 80
  emb_dropout: 0.1
  kernel_num: 80
  kernel_size: 3
  cnn_use_bn: True
  cnn_dropout: 0
  cnn_layers: 3
  # ================= Training params =================
  num_train_epochs: 40
  batch_size: 64
  optimizer: "AdamW"
  learning_rate: 0.0015
  weight_decay: 0
  gradient_accumulation_steps: 1
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  max_steps: -1
  scheduler: null
  warmup_steps: 0
BERT_CRF:
  # ================= Preprocessing params =================
  max_length: 256
  tokenizer_source: "transformers"
  tokenizer_name: "bert-base-chinese"  # source:::tokenizer_type "huggingface:::toastynews/electra-hongkongese-large-discriminator"
  add_special_tokens: False
  # ================= Model params =================
  pretrained_lm: "bert-base-chinese" # huggingface format "toastynews/electra-hongkongese-large-discriminator"
  bert_dropout: 0.1
  optimizer: "AdamW"
  batch_size: 16
  learning_rate: 2e-5
  # ================= Training params =================
  num_train_epochs: 10
  weight_decay: 0.0
  gradient_accumulation_steps: 1
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  max_steps: -1
  warmup_steps: 0
BERT_AVG:
    # ================= Preprocessing params =================
    max_length: 256
    tokenizer_source: "transformers"
    tokenizer_name: "bert-base-chinese"  # source:::tokenizer_type "huggingface:::toastynews/electra-hongkongese-large-discriminator"
    # ================= Model params =================
    pretrained_lm: "bert-base-chinese" # huggingface format "toastynews/electra-hongkongese-large-discriminator"
    dropout_rate: 0.1
    output_hidden_act_func: "ReLU"
    output_hidden_dim: 128
    sampler: "random"
    learning_rate: 2e-5
    # ================= Training params =================
    num_train_epochs: 10
    weight_decay: 0.0
    gradient_accumulation_steps: 1
    adam_epsilon: 1e-8
    max_grad_norm: 1.0
    optimizer: "AdamW"
    scheduler: "linear"
    max_steps: -1
    warmup_steps: 0
TEXT_CNN:
  # ================= Preprocessing params =================
  max_length: 256
  tokenizer_source: "internal"
  tokenizer_name: ""
  vocab_freq_cutoff: 0.015
  # ================= Model params =================
  pretrained_emb_path: null
  embedding_trainable: True
  emb_dim: 300
  emb_dropout: 0.1
  kernel_num: 256
  kernel_size: 3
  cnn_use_bn: False
  cnn_dropout: 0.2
  pool_method: "max"
  output_hidden_dim: 128
  output_hidden_act_func: "ReLU"
  output_use_bn: False
  # ================= Training params =================
  num_train_epochs: 40
  optimizer: "AdamW"
  learning_rate: 0.0015
  weight_decay: 0
  gradient_accumulation_steps: 1
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  max_steps: -1
  scheduler: null
  warmup_steps: 0
BERT_CLS:
  # ================= Preprocessing params =================
  max_length: 128
  tokenizer_source: "transformers"
  tokenizer_name: "clue/albert_chinese_tiny"  # source:::tokenizer_type "huggingface:::toastynews/electra-hongkongese-large-discriminator"
  add_special_tokens: False
  # ================= Model params =================
  pretrained_lm: "clue/albert_chinese_tiny" # huggingface format "toastynews/electra-hongkongese-large-discriminator"
  bert_dropout: 0.2
  classifier_dropout: 0.2
  optimizer: "AdamW"
  batch_size: 32
  learning_rate: 2e-5
  # ================= Training params =================
  num_train_epochs: 5
  weight_decay: 0.0
  gradient_accumulation_steps: 1
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  max_steps: -1
  warmup_steps: 0
  scheduler: linear
BERT_ATTN:
    # ================= Preprocessing params =================
    max_length: 256
    tokenizer_source: "transformers"
    tokenizer_name: "bert-base-chinese"  # source:::tokenizer_type "huggingface:::toastynews/electra-hongkongese-large-discriminator"
    # ================= Model params =================
    pretrained_lm: "bert-base-chinese" # huggingface format "toastynews/electra-hongkongese-large-discriminator"
    dropout_rate: 0.1
    output_hidden_act_func: "ReLU"
    output_hidden_dim: 128
    sampler: "random"
    learning_rate: 2e-5
    # ================= Training params =================
    num_train_epochs: 10
    weight_decay: 0.0
    gradient_accumulation_steps: 1
    adam_epsilon: 1e-8
    max_grad_norm: 1.0
    optimizer: "AdamW"
    scheduler: "linear"
    max_steps: -1
    warmup_steps: 0